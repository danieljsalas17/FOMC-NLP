{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Text Analysis\n",
    "\n",
    "In this notebook, my intention is to get a sense of what kind of inferences we can draw from different NLP techniques on Federal Reserve Press Releases. The sample dataset will contain a few scattered documents from various time periods. First, I will do some unsupervised learning, i.e. cluster analysis. Then, I will try a few supervised learning models with a few different target variables. \n",
    "\n",
    "### Unsupervised Learning Methods\n",
    "\n",
    "Cluster analysis, PCA, SVD, and possibly autoencoding NNs. \n",
    "\n",
    "### Supervised Learning Methods\n",
    "\n",
    "#### Targets\n",
    "Before considering which supervised learning methods, I need to consider which target variables are of interest. A few here that are worth trying:\n",
    "* fed funds rate\n",
    "* T-bill rates: 3m up to 10y\n",
    "* Slope of yield curve (constructed from T-bill data)\n",
    "* S&P 500 and other equity indices\n",
    "\n",
    "#### Methods\n",
    "Gradient boosted regressors (XGboost), random forest, Naive Bayes, regularized regression (Lasso), NN models\n",
    "\n",
    "### Data\n",
    "\n",
    "I am concerned with the timing of the press releases, so the data have time series structure. The tricky thing will be how to deal with press releases on the same day. I could order them by time of day as well, but even then, there may be overlap. I could append simultaneous press releases into one observation, but that may get messy. For now, I am just hoping that if I sort only by Monetary Policy press releases, I won't run into that issue. \n",
    "\n",
    "The first `fed_sample_statements.json` dataset will only contain FOMC statements going back to 2012 (got lazy, so doesn't have every statement, but . \n",
    "\n",
    "### Goals\n",
    "\n",
    "Get working text analysis models on the sample, and then move onto web scrapers to gather data other than just the FOMC statements going back to 2014. This could include the implementation notes, releases of minutes, regulation announcements, notes on strategy, etc. Eventually, I'm going to have to figure out how to use some pdf readers (not all of these are html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/fed_sample_statements.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
